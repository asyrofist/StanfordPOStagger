{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP'), (',', ','), ('the', 'DT'), ('Dutch', 'NNP'), ('publishing', 'VBG'), ('group', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(tagged_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(sentence, index):\n",
    "    return{\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'capitals_inside': False,\n",
      " 'has_hyphen': False,\n",
      " 'is_all_caps': False,\n",
      " 'is_all_lower': True,\n",
      " 'is_capitalized': False,\n",
      " 'is_first': False,\n",
      " 'is_last': True,\n",
      " 'is_numeric': False,\n",
      " 'next_word': '',\n",
      " 'prefix-1': 's',\n",
      " 'prefix-2': 'se',\n",
      " 'prefix-3': 'sen',\n",
      " 'prev_word': 'a',\n",
      " 'suffix-1': 'e',\n",
      " 'suffix-2': 'ce',\n",
      " 'suffix-3': 'nce',\n",
      " 'word': 'sentence'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(features(['This', 'is', 'a', 'sentence'], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cutoff = int(0.70 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2739\n",
      "1175\n"
     ]
    }
   ],
   "source": [
    "print(len(training_sentences))\n",
    "print(len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_to_dataset(tagged_sentences):\n",
    "    X, y = [], []\n",
    "    #X contains list of features, y contains the original tagging\n",
    "    \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    "            \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = transform_to_dataset(training_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Now we need to train classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=False)), ('classifier', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X[:10000], y[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = transform_to_dataset(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.892704007563\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_tag_predict(sentence):\n",
    "    tagged_sentence = []\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    return list(zip(sentence, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Shhe', 'PRP'), ('sells', 'VBZ'), ('seashells', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('seashore', 'VBP')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag_predict(word_tokenize('Shhe sells seashells on the seashore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('And', 'CC'), ('the', 'DT'), ('practice', 'NN'), ('should', 'MD'), (\"n't\", 'RB'), ('be', 'VB'), ('stopped', 'VBN'), ('*-1', '-NONE-'), (',', ','), ('he', 'PRP'), ('says', 'VBZ'), ('0', '-NONE-'), ('*T*-2', '-NONE-'), (',', ','), ('because', 'IN'), ('``', '``'), ('even', 'RB'), ('big', 'JJ'), ('players', 'NNS'), ('are', 'VBP'), (\"n't\", 'RB'), ('immune', 'JJ'), ('to', 'TO'), ('the', 'DT'), ('rigors', 'NNS'), ('of', 'IN'), ('program', 'NN'), ('trading', 'NN'), ('.', '.'), (\"''\", \"''\")], [('*-2', '-NONE-'), ('Also', 'RB'), ('in', 'IN'), ('New', 'NNP'), ('York', 'NNP'), (',', ','), ('Israel', 'NNP'), ('Silverman', 'NNP'), (',', ','), ('an', 'DT'), ('insurance-company', 'NN'), ('lawyer', 'NN'), (',', ','), ('comments', 'VBZ'), ('that', 'IN'), ('program', 'NN'), ('trading', 'NN'), ('``', '``'), ('increases', 'VBZ'), ('volatility', 'NN'), (',', ','), ('but', 'CC'), ('I', 'PRP'), ('do', 'VBP'), (\"n't\", 'RB'), ('think', 'VB'), ('0', '-NONE-'), ('it', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('banned', 'VBN'), ('*-1', '-NONE-'), ('.', '.')], ...]\n"
     ]
    }
   ],
   "source": [
    "print(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theseSentences = []\n",
    "for a in test_sentences:\n",
    "    theseSentTemp = [w for w, l in a]\n",
    "    theseSentJoin = ' '.join(word for word in theseSentTemp)\n",
    "    theseSentences.append(theseSentJoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictSentence = []\n",
    "for sentence in theseSentences:\n",
    "    predictSentence.append(pos_tag_predict(word_tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for i in range(len(predictSentence)):\n",
    "    if(len(predictSentence[i]) != len(test_sentences[i])):\n",
    "        idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34,\n",
       " 35,\n",
       " 38,\n",
       " 44,\n",
       " 70,\n",
       " 150,\n",
       " 151,\n",
       " 157,\n",
       " 164,\n",
       " 173,\n",
       " 189,\n",
       " 425,\n",
       " 462,\n",
       " 466,\n",
       " 468,\n",
       " 469,\n",
       " 476,\n",
       " 486,\n",
       " 489,\n",
       " 514,\n",
       " 520,\n",
       " 648,\n",
       " 649,\n",
       " 653,\n",
       " 655,\n",
       " 844,\n",
       " 930,\n",
       " 961,\n",
       " 1159,\n",
       " 1166,\n",
       " 1167]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('But', 'CC'),\n",
       " ('``', '``'),\n",
       " ('it', 'PRP'),\n",
       " ('wo', 'NN'),\n",
       " (\"n't\", 'RB'),\n",
       " ('lead', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('imminent', 'NN'),\n",
       " ('use', 'NN'),\n",
       " ('``', '``'),\n",
       " ('of', 'IN'),\n",
       " ('new', 'JJ'),\n",
       " ('superconductors', 'NNS'),\n",
       " (',', ','),\n",
       " ('cautioned', 'VBD'),\n",
       " ('0', '-NONE-'),\n",
       " ('*T*-1', '-NONE-'),\n",
       " ('Robert', 'NNP'),\n",
       " ('B.', 'NNP'),\n",
       " ('van', 'NN'),\n",
       " ('Dover', 'NNP'),\n",
       " (',', ','),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('AT', 'WDT'),\n",
       " ('&', 'CC'),\n",
       " ('T', 'NNP'),\n",
       " ('researchers', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictSentence[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('But', 'CC'),\n",
       " ('``', '``'),\n",
       " ('it', 'PRP'),\n",
       " ('wo', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('lead', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('imminent', 'JJ'),\n",
       " ('use', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('of', 'IN'),\n",
       " ('new', 'JJ'),\n",
       " ('superconductors', 'NNS'),\n",
       " (',', ','),\n",
       " ('cautioned', 'VBD'),\n",
       " ('0', '-NONE-'),\n",
       " ('*T*-1', '-NONE-'),\n",
       " ('Robert', 'NNP'),\n",
       " ('B.', 'NNP'),\n",
       " ('van', 'NNP'),\n",
       " ('Dover', 'NNP'),\n",
       " (',', ','),\n",
       " ('one', 'CD'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('AT&T', 'NNP'),\n",
       " ('researchers', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentences[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_accuracy(predict, test):\n",
    "    count = 0\n",
    "    ptags = []\n",
    "    ttags = []\n",
    "    for i in range(len(predict)): # numer of sentences\n",
    "        if (len(predict[i]) == len(test[i])): # length of sentence\n",
    "            for j in range(len(predict[i])):\n",
    "                if(predict[i][j][0] == test[i][j][0]):\n",
    "                    if(predict[i][j][1] == test[i][j][1]):\n",
    "                        count = count + 1\n",
    "            \n",
    "        elif(len(predict[i]) > len(test[i])):\n",
    "            for j in range(len(predict[i])):\n",
    "                for k in range(len(test[i])):\n",
    "                    if(predict[i][j][0] == test[i][k][0]):\n",
    "                        if(predict[i][j][1] == test[i][k][1]):\n",
    "                            count = count + 1\n",
    "                            j = j + 1\n",
    "                            if(j > len(predict[i])):\n",
    "                                break\n",
    "            \n",
    "            \n",
    "    #for p_sent1 in predict:\n",
    "    #    for tups in p_sent1:\n",
    "    #        ptags.append(tups[1])\n",
    "    #        #ptags = [n for tup in p_sent1 for m, n in tup]\n",
    "    #for t_sent1 in test:\n",
    "    #    for tups in t_sent1:\n",
    "    #        ttags.append(tups[1])\n",
    "            #ttags = [n  for m, n in tup]\n",
    "    #print(len(ptags))\n",
    "    #print(len(ttags))\n",
    "    #for i in range(len(ptags)):\n",
    "     #   if(ptags[i] == ttags[i]):\n",
    "      #      count = count + 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## WORD ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_length(predict, test):\n",
    "    for i in range(len(predict)):\n",
    "        if(len(predict[i]) > len(test[i])):\n",
    "            print(\"Predict longer than test: \", i)\n",
    "        elif(len(predict[i]) > len(test[i])):\n",
    "            print(\"Test longer than predict: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict longer than test:  34\n",
      "Predict longer than test:  35\n",
      "Predict longer than test:  38\n",
      "Predict longer than test:  44\n",
      "Predict longer than test:  70\n",
      "Predict longer than test:  150\n",
      "Predict longer than test:  151\n",
      "Predict longer than test:  157\n",
      "Predict longer than test:  164\n",
      "Predict longer than test:  173\n",
      "Predict longer than test:  189\n",
      "Predict longer than test:  425\n",
      "Predict longer than test:  462\n",
      "Predict longer than test:  466\n",
      "Predict longer than test:  468\n",
      "Predict longer than test:  469\n",
      "Predict longer than test:  476\n",
      "Predict longer than test:  486\n",
      "Predict longer than test:  489\n",
      "Predict longer than test:  514\n",
      "Predict longer than test:  520\n",
      "Predict longer than test:  648\n",
      "Predict longer than test:  649\n",
      "Predict longer than test:  653\n",
      "Predict longer than test:  655\n",
      "Predict longer than test:  844\n",
      "Predict longer than test:  930\n",
      "Predict longer than test:  961\n",
      "Predict longer than test:  1159\n",
      "Predict longer than test:  1166\n",
      "Predict longer than test:  1167\n"
     ]
    }
   ],
   "source": [
    "check_length(predictSentence, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_word_sim(predict, test):\n",
    "    count = 0\n",
    "    for i in range(len(predict)):\n",
    "        pred_list = []\n",
    "        test_list = []\n",
    "        pred_list = untag(predict[i])\n",
    "        test_list = untag(test[i])\n",
    "        for j in test_list:\n",
    "            if(j in pred_list):\n",
    "                if(predict[i][pred_list.index(j)][1] == test[i][test_list.index(j)][1]):\n",
    "                    count = count + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26257"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sim(predictSentence, test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_size(test):\n",
    "    return len([w for sent in test for w in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29619\n"
     ]
    }
   ],
   "source": [
    "print(test_size(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8864917789256896\n"
     ]
    }
   ],
   "source": [
    "print(check_word_sim(predictSentence, test_sentences) / test_size(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SENTENCE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sent_sim(predict, test):\n",
    "    count = 0\n",
    "    i = 0\n",
    "    while(i < len(predict)):\n",
    "        if(len(predict[i]) == len(test[i])):\n",
    "            if(predict[i] == test[i]):\n",
    "                count = count + 1\n",
    "        \n",
    "        pred_list = []\n",
    "        test_list = []\n",
    "        pred_list = untag(predict[i])\n",
    "        test_list = untag(test[i])\n",
    "        temp = 0\n",
    "        for j in test_list:\n",
    "            if(not (j in pred_list)):\n",
    "                break\n",
    "            elif(j in pred_list):\n",
    "                if(predict[i][pred_list.index(j)][1] == test[i][test_list.index(j)][1]):\n",
    "                    temp = temp + 1\n",
    "                if(temp == len(test[i])):\n",
    "                    count = count + 1\n",
    "        i = i + 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sent_sim(predictSentence, test_sentences) / len(predictSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1175"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29619\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
